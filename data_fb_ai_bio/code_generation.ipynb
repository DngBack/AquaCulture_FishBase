{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a990692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook generated: FishBase_AI_Biology_Pipeline_HF.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Create a fully self-contained notebook that loads FishBase from Hugging Face (cboettig/fishbase),\n",
    "# builds features, trains models, and benchmarks vs Pauly/Hoenig/Then.\n",
    "#\n",
    "# The notebook uses huggingface_hub (preferred) or DuckDB HTTPFS as a fallback.\n",
    "# It includes robust column aliasing to handle small schema differences across versions.\n",
    "#\n",
    "import nbformat as nbf\n",
    "from pathlib import Path\n",
    "\n",
    "nb = nbf.v4.new_notebook()\n",
    "cells = []\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "# FishBase × AI+Biology (HuggingFace Edition)\n",
    "**Dataset:** [`cboettig/fishbase`](https://huggingface.co/datasets/cboettig/fishbase)  •  **Focus:** Growth (K, L∞) & Natural Mortality (M) prediction  •  **Benchmarks:** Pauly (1980), Hoenig/Hewitt (2005), Then et al. (2015)\n",
    "\n",
    "This notebook provides a **reproducible pipeline**:\n",
    "1) Load FishBase tables (`species`, `ecology`, `popgrowth`, `popchar`) from **Hugging Face** (Parquet)  \n",
    "2) Build a **feature store** with biologically-meaningful variables  \n",
    "3) Train ML models (**GLM / XGBoost / CatBoost / Tabular DL-ready**)  \n",
    "4) Benchmark vs classic estimators **Pauly/Hoenig/Then** with **GroupKFold by Family**\n",
    "\n",
    "> ⚠️ Tip: The dataset repo is **versioned** (e.g., `v24.07`). Pin a specific folder or a commit `revision` for perfect reproducibility.\n",
    "\"\"\"))\n",
    "\n",
    "# Setup & Config\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 0) Setup & Config\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "# If needed, uncomment to install dependencies (run-time environment dependent)\n",
    "# %pip install -q pandas numpy scikit-learn xgboost catboost duckdb huggingface_hub pyarrow\n",
    "\n",
    "import os, math, json, itertools, typing as T, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Optional (will skip gracefully if unavailable)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "\n",
    "# Hugging Face config\n",
    "REPO_ID  = \"cboettig/fishbase\"\n",
    "FB_VER   = \"v24.07\"  # pin a version folder within the repo (e.g., v24.07)\n",
    "PARQUET_BASE = f\"data/fb/{FB_VER}/parquet\"\n",
    "\n",
    "DATA_DIR = Path(\"data_fb_ai_bio\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\"\"\"))\n",
    "\n",
    "# Data Loading\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 1) Load FishBase tables from Hugging Face\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "def load_parquet_hf(path: str) -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"Load a parquet file from Hugging Face repo using huggingface_hub.\n",
    "    If huggingface_hub is unavailable, returns None.\\\"\\\"\\\"\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        fp = hf_hub_download(repo_id=REPO_ID, filename=path)\n",
    "        return pd.read_parquet(fp)\n",
    "    except Exception as e:\n",
    "        print(\"[HF Hub] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_parquet_httpfs(path: str) -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"Load a parquet file directly via HTTP using DuckDB HTTPFS.\\\"\\\"\\\"\n",
    "    try:\n",
    "        import duckdb\n",
    "        duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        url = f\"https://huggingface.co/datasets/{REPO_ID}/resolve/main/{path}?download=true\"\n",
    "        return duckdb.sql(f\\\"\\\"\\\"SELECT * FROM read_parquet('{url}')\\\"\\\"\\\").df()\n",
    "    except Exception as e:\n",
    "        print(\"[DuckDB HTTPFS] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_fb_table(name: str) -> pd.DataFrame:\n",
    "    rel = f\\\"{PARQUET_BASE}/{name}.parquet\\\"\n",
    "    df = load_parquet_hf(rel)\n",
    "    if df is None:\n",
    "        df = load_parquet_httpfs(rel)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Cannot load {name} via HF or HTTPFS. Please check connectivity or install deps.\")\n",
    "    return df\n",
    "\n",
    "species  = load_fb_table(\"species\")\n",
    "ecology  = load_fb_table(\"ecology\")\n",
    "popgrowth= load_fb_table(\"popgrowth\")\n",
    "popchar  = load_fb_table(\"popchar\")\n",
    "\n",
    "species.head(), ecology.head(), popgrowth.head(), popchar.head()\n",
    "\"\"\"))\n",
    "\n",
    "# Schema & Aliases\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 2) Schema checks & column aliases\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def first_col(df, candidates: T.List[str], required=False):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of {candidates} found in columns: {list(df.columns)[:20]}...\")\n",
    "    return None\n",
    "\n",
    "# Aliases to handle minor schema differences across FishBase snapshots\n",
    "ALIASES = {\n",
    "    \"SpecCode\": [\"SpecCode\", \"SpecCode_x\", \"SpecCode_y\"],\n",
    "    \"Family\": [\"Family\"],\n",
    "    \"Order\": [\"Order\"],\n",
    "    \"Class\": [\"Class\"],\n",
    "    \"BodyShapeI\": [\"BodyShapeI\", \"BodyShape\"],\n",
    "    \"DemersPelag\": [\"DemersPelag\", \"DemersPelagics\"],\n",
    "    \"AnaCat\": [\"AnaCat\"],\n",
    "    \"EnvTemp\": [\"EnvTemp\"],\n",
    "    \"DepthRangeShallow\": [\"DepthRangeShallow\", \"DepthShallow\"],\n",
    "    \"DepthRangeDeep\": [\"DepthRangeDeep\", \"DepthDeep\"],\n",
    "    \"Fresh\": [\"Fresh\"],\n",
    "    \"Brack\": [\"Brack\"],\n",
    "    \"Saltwater\": [\"Saltwater\"],\n",
    "    \"Length\": [\"Length\"],\n",
    "    \"LTypeMaxM\": [\"LTypeMaxM\"],\n",
    "    \"Weight\": [\"Weight\"],\n",
    "    \"LongevityWild\": [\"LongevityWild\", \"Longevity\"],\n",
    "\n",
    "    # Ecology\n",
    "    \"FoodTroph\": [\"FoodTroph\"],\n",
    "    \"DietTroph\": [\"DietTroph\"],\n",
    "    \"FoodSeTroph\": [\"FoodSeTroph\"],\n",
    "    \"DietSeTroph\": [\"DietSeTroph\"],\n",
    "\n",
    "    # Popgrowth\n",
    "    \"Loo\": [\"Loo\", \"Linf\", \"Linf_cm\", \"Linf_cm_\", \"Linfinity\"],\n",
    "    \"K\": [\"K\"],\n",
    "    \"to\": [\"to\", \"t0\"],\n",
    "    \"M\": [\"M\"],\n",
    "    \"tm\": [\"tm\", \"tm50\"],\n",
    "    \"Lm\": [\"Lm\", \"Lm50\"],\n",
    "\n",
    "    # Popchar\n",
    "    \"tmax\": [\"tmax\",\"Tmax\",\"LongevityWild\"],  # fallback if tmax absent\n",
    "    \"Lmax\": [\"Lmax\"],\n",
    "    \"Wmax\": [\"Wmax\"],\n",
    "}\n",
    "\n",
    "def alias(df: pd.DataFrame, key: str, required=False):\n",
    "    return first_col(df, ALIASES[key], required=required)\n",
    "\n",
    "# Keep a copy of original column names for reference\n",
    "orig_cols = {\n",
    "    \"species\": list(species.columns),\n",
    "    \"ecology\": list(ecology.columns),\n",
    "    \"popgrowth\": list(popgrowth.columns),\n",
    "    \"popchar\": list(popchar.columns),\n",
    "}\n",
    "orig_cols\n",
    "\"\"\"))\n",
    "\n",
    "# Feature Engineering\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 3) Feature engineering & targets\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def map_envtemp_to_T(envtemp: str) -> float:\n",
    "    m = {\"Tropical\":27.0, \"Subtropical\":20.0, \"Temperate\":12.0, \"Polar\":2.0}\n",
    "    if pd.isna(envtemp):\n",
    "        return np.nan\n",
    "    return m.get(str(envtemp).strip().title(), np.nan)\n",
    "\n",
    "# Coerce key dtypes & select columns by aliases\n",
    "def coerce_numeric(df: pd.DataFrame, cols: T.List[str]):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# Build unified feature table\n",
    "def build_feature_store(species, ecology, popgrowth, popchar) -> pd.DataFrame:\n",
    "    sp = species.copy()\n",
    "    ec = ecology.copy()\n",
    "    pg = popgrowth.copy()\n",
    "    pc = popchar.copy()\n",
    "\n",
    "    # Coerce IDs\n",
    "    sc = alias(sp, \"SpecCode\", required=True)\n",
    "    for df in (sp, ec, pg, pc):\n",
    "        if alias(df, \"SpecCode\") is None:\n",
    "            continue\n",
    "        df[alias(df, \"SpecCode\")] = pd.to_numeric(df[alias(df, \"SpecCode\")], errors=\"coerce\")\n",
    "\n",
    "    # Reduce to needed columns (by alias lookup)\n",
    "    sp_cols = [x for k in [\"SpecCode\",\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\n",
    "                           \"DepthRangeShallow\",\"DepthRangeDeep\",\"Fresh\",\"Brack\",\"Saltwater\",\"Length\",\"LTypeMaxM\",\n",
    "                           \"Weight\",\"LongevityWild\"] if (x:=alias(sp,k))] + []\n",
    "    ec_cols = [x for k in [\"SpecCode\",\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"] if (x:=alias(ec,k))]\n",
    "    pg_cols = [x for k in [\"SpecCode\",\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"] if (x:=alias(pg,k))]\n",
    "    pc_cols = [x for k in [\"SpecCode\",\"tmax\",\"Lmax\",\"Wmax\"] if (x:=alias(pc,k))]\n",
    "\n",
    "    sp1 = sp[sp_cols].copy()\n",
    "    ec1 = ec[ec_cols].copy() if ec_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pg1 = pg[pg_cols].copy() if pg_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pc1 = pc[pc_cols].copy() if pc_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "\n",
    "    # Standardize names\n",
    "    def stdcol(df, key):\n",
    "        c = alias(df, key)\n",
    "        if c and c != key:\n",
    "            df = df.rename(columns={c:key})\n",
    "        return df\n",
    "\n",
    "    for key in [\"SpecCode\",\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\n",
    "                \"DepthRangeShallow\",\"DepthRangeDeep\",\"Fresh\",\"Brack\",\"Saltwater\",\"Length\",\"LTypeMaxM\",\n",
    "                \"Weight\",\"LongevityWild\"]:\n",
    "        sp1 = stdcol(sp1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"]:\n",
    "        ec1 = stdcol(ec1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"]:\n",
    "        pg1 = stdcol(pg1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"tmax\",\"Lmax\",\"Wmax\"]:\n",
    "        pc1 = stdcol(pc1, key)\n",
    "\n",
    "    # Numeric coercion\n",
    "    sp1 = coerce_numeric(sp1, [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"LongevityWild\",\n",
    "                               \"Fresh\",\"Brack\",\"Saltwater\"])\n",
    "    ec1 = coerce_numeric(ec1, [\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"])\n",
    "    pg1 = coerce_numeric(pg1, [\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"])\n",
    "    pc1 = coerce_numeric(pc1, [\"tmax\",\"Lmax\",\"Wmax\"])\n",
    "\n",
    "    # Merge tables on SpecCode\n",
    "    df = sp1.merge(ec1, on=\"SpecCode\", how=\"left\")\n",
    "    df = df.merge(pg1, on=\"SpecCode\", how=\"left\")\n",
    "    df = df.merge(pc1, on=\"SpecCode\", how=\"left\")\n",
    "\n",
    "    # Derived features\n",
    "    df[\"T_proxy\"] = df[\"EnvTemp\"].map(map_envtemp_to_T)\n",
    "\n",
    "    # Targets (log-scale)\n",
    "    for t in [\"K\",\"Loo\",\"M\"]:\n",
    "        if t in df.columns:\n",
    "            df[f\"log_{t}\"] = np.log(df[t].astype(float))\n",
    "\n",
    "    # tmax_any: prefer tmax, fallback LongevityWild\n",
    "    if \"tmax\" in df.columns:\n",
    "        df[\"tmax_any\"] = df[\"tmax\"].where(~df[\"tmax\"].isna(), df.get(\"LongevityWild\", np.nan))\n",
    "    else:\n",
    "        df[\"tmax_any\"] = df.get(\"LongevityWild\", np.nan)\n",
    "\n",
    "    # Categoricals\n",
    "    for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    # Save feature store\n",
    "    out_path = DATA_DIR / \"feature_store.parquet\"\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(\"Feature store saved:\", out_path, \"rows:\", len(df))\n",
    "    return df\n",
    "\n",
    "feature_store = build_feature_store(species, ecology, popgrowth, popchar)\n",
    "feature_store.head()\n",
    "\"\"\"))\n",
    "\n",
    "# Baselines\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 4) Baseline estimators (Pauly, Hoenig/Hewitt, Then)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def pauly_M(K, Linf_cm, T_celsius):\n",
    "    if any(pd.isna(x) for x in [K, Linf_cm, T_celsius]):\n",
    "        return np.nan\n",
    "    val = -0.0066 - 0.279*math.log10(Linf_cm) + 0.6543*math.log10(K) + 0.4634*math.log10(T_celsius)\n",
    "    return 10 ** val\n",
    "\n",
    "def hoenig_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0:\n",
    "        return np.nan\n",
    "    return 4.22 / tmax  # Hewitt & Hoenig (2005) rule-of-thumb\n",
    "\n",
    "def then_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0:\n",
    "        return np.nan\n",
    "    return 4.899 * (tmax ** -0.916)\n",
    "\n",
    "def then_M_from_growth(K, Linf_cm):\n",
    "    if any(pd.isna(x) or x<=0 for x in [K, Linf_cm]):\n",
    "        return np.nan\n",
    "    return 1.521 * (K ** 0.72) * (Linf_cm ** -0.33)\n",
    "\"\"\"))\n",
    "\n",
    "# Splits & Metrics\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 5) Splits & metrics\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def group_splits(df: pd.DataFrame, group_col=\"Family\", n_splits=5):\n",
    "    df_ = df.dropna(subset=[group_col]).copy()\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = df_[group_col].astype(str).values\n",
    "    for fold, (tr, te) in enumerate(gkf.split(df_, groups=groups)):\n",
    "        yield fold, df_.iloc[tr].copy(), df_.iloc[te].copy()\n",
    "\n",
    "def metrics_log(y_true_log, y_pred_log):\n",
    "    rmse = math.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2  = r2_score(y_true_log, y_pred_log)\n",
    "    # MAPE on original scale\n",
    "    y_true = np.exp(y_true_log)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))))\n",
    "    return dict(rmse_log=rmse, mae_log=mae, r2=r2, mape=mape)\n",
    "\"\"\"))\n",
    "\n",
    "# Models\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 6) Models (GLM / XGBoost / CatBoost)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def fit_glm_numeric(X_train, y_train_log, num_cols):\n",
    "    pre = ColumnTransformer([(\"num\", StandardScaler(), num_cols)], remainder=\"drop\")\n",
    "    model = Pipeline([(\"prep\", pre), (\"lin\", LinearRegression())])\n",
    "    model.fit(X_train[num_cols], y_train_log)\n",
    "    return model\n",
    "\n",
    "def fit_xgb(train_df, y_train_log, cat_cols, num_cols):\n",
    "    if xgb is None:\n",
    "        return None\n",
    "    X = pd.get_dummies(train_df[cat_cols+num_cols], drop_first=False)\n",
    "    dtrain = xgb.DMatrix(X, label=y_train_log)\n",
    "    params = dict(objective=\"reg:squarederror\", eval_metric=\"rmse\",\n",
    "                  eta=0.03, max_depth=8, subsample=0.8, colsample_bytree=0.8, seed=RANDOM_SEED)\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=400, verbose_eval=False)\n",
    "    return bst, X.columns.tolist()\n",
    "\n",
    "def predict_xgb(model_tuple, X_df):\n",
    "    bst, cols = model_tuple\n",
    "    X = pd.get_dummies(X_df, drop_first=False)\n",
    "    for c in cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    X = X[cols]\n",
    "    dtest = xgb.DMatrix(X)\n",
    "    return bst.predict(dtest)\n",
    "\n",
    "def fit_catboost(train_df, y_train_log, cat_cols, num_cols):\n",
    "    if CatBoostRegressor is None:\n",
    "        return None\n",
    "    X = train_df[cat_cols+num_cols].copy()\n",
    "    cat_idx = [X.columns.get_loc(c) for c in cat_cols]\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\", depth=8, learning_rate=0.05, l2_leaf_reg=6, iterations=400,\n",
    "        verbose=False, random_seed=RANDOM_SEED\n",
    "    )\n",
    "    model.fit(X, y_train_log, cat_features=cat_idx)\n",
    "    return model\n",
    "\"\"\"))\n",
    "\n",
    "# Experiment B\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 7) Experiment **Task B** — Predict `log_M`\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "df = pd.read_parquet(DATA_DIR/\"feature_store.parquet\")\n",
    "\n",
    "cat_cols = [c for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"] if c in df.columns]\n",
    "num_base = [c for c in [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"FoodTroph\",\"DietTroph\"] if c in df.columns]\n",
    "\n",
    "results = []\n",
    "task = \"B\"\n",
    "target = \"log_M\"\n",
    "\n",
    "scenarios = {\n",
    "    \"B_full\": {\"extra_num\": [\"K\",\"Loo\",\"T_proxy\"], \"require\": [\"log_M\"]},\n",
    "    \"B_lite\": {\"extra_num\": [], \"require\": [\"log_M\"]}\n",
    "}\n",
    "\n",
    "def compute_baseline_logs(test):\n",
    "    p = test.apply(lambda r: pauly_M(r.get(\"K\"), r.get(\"Loo\"), r.get(\"T_proxy\")), axis=1)\n",
    "    h = test.apply(lambda r: hoenig_M_from_tmax(r.get(\"tmax_any\")), axis=1)\n",
    "    t1= test.apply(lambda r: then_M_from_tmax(r.get(\"tmax_any\")), axis=1)\n",
    "    t2= test.apply(lambda r: then_M_from_growth(r.get(\"K\"), r.get(\"Loo\")), axis=1)\n",
    "    return np.log(p), np.log(h), np.log(t1), np.log(t2)\n",
    "\n",
    "for scen, cfg in scenarios.items():\n",
    "    cols = num_base + [c for c in cfg[\"extra_num\"] if c in df.columns]\n",
    "    dsub = df.dropna(subset=cfg[\"require\"]).copy()\n",
    "\n",
    "    for fold, train, test in group_splits(dsub, group_col=\"Family\", n_splits=5):\n",
    "        y_tr = train[target].values\n",
    "        y_te = test[target].values\n",
    "\n",
    "        # GLM numeric\n",
    "        glm = fit_glm_numeric(train, y_tr, num_cols=cols)\n",
    "        yhat_glm = glm.predict(test[cols])\n",
    "\n",
    "        # XGB\n",
    "        if xgb is not None:\n",
    "            xgb_model = fit_xgb(train, y_tr, cat_cols, cols)\n",
    "            yhat_xgb = predict_xgb(xgb_model, test[cat_cols+cols])\n",
    "        else:\n",
    "            yhat_xgb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        # CatBoost\n",
    "        if CatBoostRegressor is not None:\n",
    "            cb = fit_catboost(train, y_tr, cat_cols, cols)\n",
    "            yhat_cb = cb.predict(test[cat_cols+cols])\n",
    "        else:\n",
    "            yhat_cb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        pauly_log, hoenig_log, then_tmax_log, then_growth_log = compute_baseline_logs(test)\n",
    "\n",
    "        for name, pred in {\"GLM\":yhat_glm, \"XGBoost\":yhat_xgb, \"CatBoost\":yhat_cb}.items():\n",
    "            m = metrics_log(y_te, pred)\n",
    "            # ΔRMSE vs baselines\n",
    "            def rmse_of(b):\n",
    "                mask = ~np.isnan(b)\n",
    "                return math.sqrt(mean_squared_error(y_te[mask], b[mask]))\n",
    "            try: d_pauly = m[\"rmse_log\"] - rmse_of(pauly_log.values if hasattr(pauly_log,\"values\") else pauly_log)\n",
    "            except: d_pauly = np.nan\n",
    "            try: d_hoenig = m[\"rmse_log\"] - rmse_of(hoenig_log.values if hasattr(hoenig_log,\"values\") else hoenig_log)\n",
    "            except: d_hoenig = np.nan\n",
    "            try: d_then1 = m[\"rmse_log\"] - rmse_of(then_tmax_log.values if hasattr(then_tmax_log,\"values\") else then_tmax_log)\n",
    "            except: d_then1 = np.nan\n",
    "            try: d_then2 = m[\"rmse_log\"] - rmse_of(then_growth_log.values if hasattr(then_growth_log,\"values\") else then_growth_log)\n",
    "            except: d_then2 = np.nan\n",
    "\n",
    "            results.append(dict(task=task, scenario=scen, target=target, fold=fold, model=name, **m,\n",
    "                                delta_rmse_vs_pauly=d_pauly, delta_rmse_vs_hoenig=d_hoenig,\n",
    "                                delta_rmse_vs_then_tmax=d_then1, delta_rmse_vs_then_growth=d_then2,\n",
    "                                n_test=int(len(y_te))))\n",
    "\n",
    "resB = pd.DataFrame(results).sort_values([\"scenario\",\"model\",\"fold\"])\n",
    "resB_path = DATA_DIR / \"benchmark_results_M.csv\"\n",
    "resB.to_csv(resB_path, index=False)\n",
    "resB.head()\n",
    "\"\"\"))\n",
    "\n",
    "# Experiment A\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 8) Experiment **Task A** — Predict `log_K` & `log_Loo`\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "df = pd.read_parquet(DATA_DIR/\"feature_store.parquet\")\n",
    "cat_cols = [c for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"] if c in df.columns]\n",
    "num_cols = [c for c in [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"FoodTroph\",\"DietTroph\",\"T_proxy\"] if c in df.columns]\n",
    "\n",
    "targets = [t for t in [\"log_K\",\"log_Loo\"] if t in df.columns]\n",
    "rows = []\n",
    "for target in targets:\n",
    "    dsub = df.dropna(subset=[target]).copy()\n",
    "    for fold, train, test in group_splits(dsub, group_col=\"Family\", n_splits=5):\n",
    "        y_tr = train[target].values\n",
    "        y_te = test[target].values\n",
    "\n",
    "        glm = fit_glm_numeric(train, y_tr, num_cols=num_cols)\n",
    "        yhat_glm = glm.predict(test[num_cols])\n",
    "\n",
    "        if xgb is not None:\n",
    "            xgb_model = fit_xgb(train, y_tr, cat_cols, num_cols)\n",
    "            yhat_xgb = predict_xgb(xgb_model, test[cat_cols+num_cols])\n",
    "        else:\n",
    "            yhat_xgb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        if CatBoostRegressor is not None:\n",
    "            cb = fit_catboost(train, y_tr, cat_cols, num_cols)\n",
    "            yhat_cb = cb.predict(test[cat_cols+num_cols])\n",
    "        else:\n",
    "            yhat_cb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        for name, pred in {\"GLM\":yhat_glm, \"XGBoost\":yhat_xgb, \"CatBoost\":yhat_cb}.items():\n",
    "            m = metrics_log(y_te, pred)\n",
    "            rows.append(dict(task=\"A\", target=target, fold=fold, model=name, **m, n_test=int(len(y_te))))\n",
    "\n",
    "resA = pd.DataFrame(rows).sort_values([\"target\",\"model\",\"fold\"])\n",
    "resA_path = DATA_DIR / \"benchmark_results_growth.csv\"\n",
    "resA.to_csv(resA_path, index=False)\n",
    "resA.head()\n",
    "\"\"\"))\n",
    "\n",
    "# Reporting\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 9) Reporting — aggregated tables & quick plots\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def summarize(res, keys=[\"scenario\",\"target\",\"model\"]):\n",
    "    grp = res.groupby(keys).agg(\n",
    "        rmse_log=(\"rmse_log\",\"mean\"),\n",
    "        mae_log=(\"mae_log\",\"mean\"),\n",
    "        r2=(\"r2\",\"mean\"),\n",
    "        mape=(\"mape\",\"mean\"),\n",
    "        n=(\"n_test\",\"sum\")\n",
    "    ).reset_index()\n",
    "    return grp\n",
    "\n",
    "# Summaries\n",
    "resB = pd.read_csv(DATA_DIR/\"benchmark_results_M.csv\")\n",
    "resA = pd.read_csv(DATA_DIR/\"benchmark_results_growth.csv\")\n",
    "\n",
    "sumB = summarize(resB, keys=[\"scenario\",\"model\"])\n",
    "sumA = summarize(resA, keys=[\"target\",\"model\"])\n",
    "\n",
    "display(sumB.head(10))\n",
    "display(sumA.head(10))\n",
    "\n",
    "# Example plot: RMSE by model (Task B, scenario B_full)\n",
    "sub = resB[resB[\"scenario\"]==\"B_full\"].groupby(\"model\")[\"rmse_log\"].mean().reset_index()\n",
    "plt.figure()\n",
    "plt.bar(sub[\"model\"], sub[\"rmse_log\"])\n",
    "plt.title(\"Task B (B_full): mean RMSE(log) by model\")\n",
    "plt.ylabel(\"RMSE(log)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\"\"\"))\n",
    "\n",
    "# Save notebook\n",
    "nb[\"cells\"] = cells\n",
    "nb_path = \"FishBase_AI_Biology_Pipeline_HF.ipynb\"\n",
    "with open(nb_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "print(\"Notebook generated:\", nb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da546b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched notebook saved: FishBase_AI_Biology_Pipeline_HF_patched.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Create a patched notebook that is more robust to missing/renamed SpecCode in some tables.\n",
    "# Key changes vs previous version:\n",
    "# - alias() now does case-insensitive matching.\n",
    "# - Safe merge: only merge a table if it contains 'SpecCode' after aliasing/renaming; otherwise skip with a warning.\n",
    "# - Fallback: if popchar cannot be merged, set df['tmax']=NaN and rely on LongevityWild for tmax_any.\n",
    "import nbformat as nbf\n",
    "from pathlib import Path\n",
    "\n",
    "nb = nbf.v4.new_notebook()\n",
    "cells = []\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"\"\"\n",
    "# FishBase × AI+Biology (HuggingFace Edition) — **Patched**\n",
    "**Fixes:** Robust handling when `popchar` (or other tables) lacks `SpecCode` due to schema differences, avoiding `KeyError: 'SpecCode'` during merge.\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 0) Setup & Config\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "# %pip install -q pandas numpy scikit-learn xgboost catboost duckdb huggingface_hub pyarrow\n",
    "import os, math, json, itertools, typing as T, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "\n",
    "REPO_ID  = \"cboettig/fishbase\"\n",
    "FB_VER   = \"v24.07\"\n",
    "PARQUET_BASE = f\"data/fb/{FB_VER}/parquet\"\n",
    "\n",
    "DATA_DIR = Path(\"data_fb_ai_bio\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 1) Load FishBase tables\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def load_parquet_hf(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        fp = hf_hub_download(repo_id=REPO_ID, filename=path)\n",
    "        return pd.read_parquet(fp)\n",
    "    except Exception as e:\n",
    "        print(\"[HF Hub] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_parquet_httpfs(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        import duckdb\n",
    "        duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        url = f\"https://huggingface.co/datasets/{REPO_ID}/resolve/main/{path}?download=true\"\n",
    "        return duckdb.sql(f\"SELECT * FROM read_parquet('{url}')\").df()\n",
    "    except Exception as e:\n",
    "        print(\"[DuckDB HTTPFS] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_fb_table(name: str) -> pd.DataFrame:\n",
    "    rel = f\"{PARQUET_BASE}/{name}.parquet\"\n",
    "    df = load_parquet_hf(rel)\n",
    "    if df is None:\n",
    "        df = load_parquet_httpfs(rel)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Cannot load {name}. Check internet/dependencies.\")\n",
    "    return df\n",
    "\n",
    "species  = load_fb_table(\"species\")\n",
    "ecology  = load_fb_table(\"ecology\")\n",
    "popgrowth= load_fb_table(\"popgrowth\")\n",
    "popchar  = load_fb_table(\"popchar\")\n",
    "\n",
    "{tbl: list(df.columns)[:12] for tbl, df in dict(species=species, ecology=ecology, popgrowth=popgrowth, popchar=popchar).items()}\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 2) Schema aliases (case-insensitive) & helpers\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def first_col(df, candidates: T.List[str], required=False):\n",
    "    # exact first\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    # case-insensitive\n",
    "    lcmap = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lcmap: return lcmap[c.lower()]\n",
    "    if required:\n",
    "        raise KeyError(f\"None of {candidates} found in columns (sample): {list(df.columns)[:20]}\")\n",
    "    return None\n",
    "\n",
    "ALIASES = {\n",
    "    \"SpecCode\": [\"SpecCode\", \"SpecCode_x\", \"SpecCode_y\", \"speccode\", \"SpeciesCode\", \"speciescode\"],\n",
    "    \"Family\": [\"Family\"], \"Order\": [\"Order\"], \"Class\": [\"Class\"],\n",
    "    \"BodyShapeI\": [\"BodyShapeI\",\"BodyShape\"],\n",
    "    \"DemersPelag\": [\"DemersPelag\",\"DemersPelagics\"],\n",
    "    \"AnaCat\": [\"AnaCat\"], \"EnvTemp\": [\"EnvTemp\"],\n",
    "    \"DepthRangeShallow\": [\"DepthRangeShallow\",\"DepthShallow\"],\n",
    "    \"DepthRangeDeep\": [\"DepthRangeDeep\",\"DepthDeep\"],\n",
    "    \"Fresh\": [\"Fresh\"], \"Brack\": [\"Brack\"], \"Saltwater\": [\"Saltwater\"],\n",
    "    \"Length\": [\"Length\"], \"LTypeMaxM\": [\"LTypeMaxM\"], \"Weight\": [\"Weight\"],\n",
    "    \"LongevityWild\": [\"LongevityWild\",\"Longevity\"],\n",
    "    \"FoodTroph\": [\"FoodTroph\"], \"DietTroph\": [\"DietTroph\"],\n",
    "    \"FoodSeTroph\": [\"FoodSeTroph\"], \"DietSeTroph\": [\"DietSeTroph\"],\n",
    "    \"Loo\": [\"Loo\",\"Linf\",\"Linf_cm\",\"L_infinity\",\"Linfinity\"],\n",
    "    \"K\": [\"K\"], \"to\": [\"to\",\"t0\"], \"M\": [\"M\"], \"tm\": [\"tm\",\"tm50\"], \"Lm\": [\"Lm\",\"Lm50\"],\n",
    "    \"tmax\": [\"tmax\",\"Tmax\",\"LongevityWild\"], \"Lmax\": [\"Lmax\"], \"Wmax\": [\"Wmax\"],\n",
    "}\n",
    "\n",
    "def alias(df: pd.DataFrame, key: str, required=False):\n",
    "    return first_col(df, ALIASES[key], required=required)\n",
    "\n",
    "def std_rename(df: pd.DataFrame, key_list: T.List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for key in key_list:\n",
    "        c = alias(out, key, required=False)\n",
    "        if c and c != key:\n",
    "            out = out.rename(columns={c: key})\n",
    "    return out\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame, cols: T.List[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\"\"\"))\n",
    "\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 3) Build feature store (with **safe merges**)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def map_envtemp_to_T(envtemp: str) -> float:\n",
    "    m = {\"Tropical\":27.0, \"Subtropical\":20.0, \"Temperate\":12.0, \"Polar\":2.0}\n",
    "    if pd.isna(envtemp): return np.nan\n",
    "    return m.get(str(envtemp).strip().title(), np.nan)\n",
    "\n",
    "def build_feature_store(species, ecology, popgrowth, popchar) -> pd.DataFrame:\n",
    "    # Select & standardize columns\n",
    "    sp_keys = [\"SpecCode\",\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\n",
    "               \"DepthRangeShallow\",\"DepthRangeDeep\",\"Fresh\",\"Brack\",\"Saltwater\",\"Length\",\"LTypeMaxM\",\n",
    "               \"Weight\",\"LongevityWild\"]\n",
    "    ec_keys = [\"SpecCode\",\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"]\n",
    "    pg_keys = [\"SpecCode\",\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"]\n",
    "    pc_keys = [\"SpecCode\",\"tmax\",\"Lmax\",\"Wmax\"]\n",
    "\n",
    "    sp = std_rename(species[ [alias(species,k) for k in sp_keys if alias(species,k)] ], sp_keys)\n",
    "    ec = std_rename(ecology[ [alias(ecology,k) for k in ec_keys if alias(ecology,k)] ], ec_keys) if any(alias(ecology,k) for k in ec_keys) else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pg = std_rename(popgrowth[[alias(popgrowth,k) for k in pg_keys if alias(popgrowth,k)]], pg_keys) if any(alias(popgrowth,k) for k in pg_keys) else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pc = std_rename(popchar[[alias(popchar,k) for k in pc_keys if alias(popchar,k)]], pc_keys) if any(alias(popchar,k) for k in pc_keys) else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "\n",
    "    # Coerce numerics\n",
    "    sp = coerce_numeric(sp, [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"LongevityWild\",\"Fresh\",\"Brack\",\"Saltwater\"])\n",
    "    ec = coerce_numeric(ec, [\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"])\n",
    "    pg = coerce_numeric(pg, [\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"])\n",
    "    pc = coerce_numeric(pc, [\"tmax\",\"Lmax\",\"Wmax\"])\n",
    "\n",
    "    # Merge with safeguards\n",
    "    df = sp.copy()\n",
    "    if \"SpecCode\" in ec.columns:\n",
    "        df = df.merge(ec, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in ecology -> skip merge.\")\n",
    "    if \"SpecCode\" in pg.columns:\n",
    "        df = df.merge(pg, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in popgrowth -> skip merge.\")\n",
    "    if \"SpecCode\" in pc.columns:\n",
    "        df = df.merge(pc, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in popchar -> skip merge; using LongevityWild as fallback for tmax_any.\")\n",
    "        df[\"tmax\"] = np.nan\n",
    "\n",
    "    # Derived\n",
    "    df[\"T_proxy\"] = df.get(\"EnvTemp\", pd.Series(index=df.index)).map(map_envtemp_to_T)\n",
    "    for t in [\"K\",\"Loo\",\"M\"]:\n",
    "        if t in df.columns:\n",
    "            df[f\"log_{t}\"] = np.log(df[t].astype(float))\n",
    "    df[\"tmax_any\"] = df[\"tmax\"] if \"tmax\" in df.columns else np.nan\n",
    "    if \"LongevityWild\" in df.columns:\n",
    "        df[\"tmax_any\"] = df[\"tmax_any\"].fillna(df[\"LongevityWild\"])\n",
    "\n",
    "    for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    out_path = DATA_DIR / \"feature_store.parquet\"\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(\"Feature store saved:\", out_path, \"rows:\", len(df))\n",
    "    return df\n",
    "\n",
    "feature_store = build_feature_store(species, ecology, popgrowth, popchar)\n",
    "feature_store.head()\n",
    "\"\"\"))\n",
    "\n",
    "# Baselines & Experiments copied (shortened to essential to keep file size modest)\n",
    "cells.append(nbf.v4.new_markdown_cell(\"## 4) Baselines & Experiments (same as previous notebook)\"))\n",
    "cells.append(nbf.v4.new_code_cell(\"\"\"\n",
    "def pauly_M(K, Linf_cm, T_celsius):\n",
    "    if any(pd.isna(x) for x in [K, Linf_cm, T_celsius]): return np.nan\n",
    "    val = -0.0066 - 0.279*np.log10(Linf_cm) + 0.6543*np.log10(K) + 0.4634*np.log10(T_celsius)\n",
    "    return 10 ** val\n",
    "\n",
    "def hoenig_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0: return np.nan\n",
    "    return 4.22 / tmax\n",
    "\n",
    "def then_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0: return np.nan\n",
    "    return 4.899 * (tmax ** -0.916)\n",
    "\n",
    "def then_M_from_growth(K, Linf_cm):\n",
    "    if any(pd.isna(x) or x<=0 for x in [K, Linf_cm]): return np.nan\n",
    "    return 1.521 * (K ** 0.72) * (Linf_cm ** -0.33)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math, numpy as np, pandas as pd\n",
    "\n",
    "def group_splits(df: pd.DataFrame, group_col=\"Family\", n_splits=5):\n",
    "    df_ = df.dropna(subset=[group_col]).copy()\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = df_[group_col].astype(str).values\n",
    "    for fold, (tr, te) in enumerate(gkf.split(df_, groups=groups)):\n",
    "        yield fold, df_.iloc[tr].copy(), df_.iloc[te].copy()\n",
    "\n",
    "def metrics_log(y_true_log, y_pred_log):\n",
    "    rmse = math.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae  = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2   = r2_score(y_true_log, y_pred_log)\n",
    "    y_true = np.exp(y_true_log); y_pred = np.exp(y_pred_log)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred)/np.clip(np.abs(y_true),1e-8,None))))\n",
    "    return dict(rmse_log=rmse, mae_log=mae, r2=r2, mape=mape)\n",
    "\n",
    "def fit_glm_numeric(X_train, y_train_log, num_cols):\n",
    "    pre = ColumnTransformer([(\"num\", StandardScaler(), num_cols)], remainder=\"drop\")\n",
    "    model = Pipeline([(\"prep\", pre), (\"lin\", LinearRegression())])\n",
    "    model.fit(X_train[num_cols], y_train_log); return model\n",
    "\n",
    "def fit_xgb(train_df, y_train_log, cat_cols, num_cols):\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception:\n",
    "        return None\n",
    "    X = pd.get_dummies(train_df[cat_cols+num_cols], drop_first=False)\n",
    "    dtr = xgb.DMatrix(X, label=y_train_log)\n",
    "    params = dict(objective=\"reg:squarederror\", eval_metric=\"rmse\", eta=0.03, max_depth=8,\n",
    "                  subsample=0.8, colsample_bytree=0.8, seed=42)\n",
    "    bst = xgb.train(params, dtr, num_boost_round=400, verbose_eval=False)\n",
    "    return bst, X.columns.tolist()\n",
    "\n",
    "def predict_xgb(model_tuple, X_df):\n",
    "    bst, cols = model_tuple\n",
    "    X = pd.get_dummies(X_df, drop_first=False)\n",
    "    for c in cols:\n",
    "        if c not in X.columns: X[c] = 0\n",
    "    X = X[cols]\n",
    "    import xgboost as xgb\n",
    "    dte = xgb.DMatrix(X)\n",
    "    return bst.predict(dte)\n",
    "\n",
    "def fit_catboost(train_df, y_train_log, cat_cols, num_cols):\n",
    "    try:\n",
    "        from catboost import CatBoostRegressor\n",
    "    except Exception:\n",
    "        return None\n",
    "    X = train_df[cat_cols+num_cols].copy()\n",
    "    cat_idx = [X.columns.get_loc(c) for c in cat_cols]\n",
    "    model = CatBoostRegressor(loss_function=\"RMSE\", depth=8, learning_rate=0.05, l2_leaf_reg=6,\n",
    "                              iterations=400, verbose=False, random_seed=42)\n",
    "    model.fit(X, y_train_log, cat_features=cat_idx); return model\n",
    "\n",
    "# Run a quick M-task fold to validate pipeline (can expand to full CV)\n",
    "df = pd.read_parquet(Path(\"data_fb_ai_bio\")/\"feature_store.parquet\")\n",
    "cat_cols = [c for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"] if c in df.columns]\n",
    "num_base = [c for c in [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"FoodTroph\",\"DietTroph\"] if c in df.columns]\n",
    "cols = num_base + [c for c in [\"K\",\"Loo\",\"T_proxy\"] if c in df.columns]\n",
    "\n",
    "dsub = df.dropna(subset=[\"log_M\"]).copy()\n",
    "fold, train, test = next(group_splits(dsub, group_col=\"Family\", n_splits=5))\n",
    "y_tr = train[\"log_M\"].values; y_te = test[\"log_M\"].values\n",
    "\n",
    "glm = fit_glm_numeric(train, y_tr, num_cols=cols)\n",
    "yhat_glm = glm.predict(test[cols])\n",
    "\n",
    "try:\n",
    "    xgb_model = fit_xgb(train, y_tr, cat_cols, cols)\n",
    "    yhat_xgb = predict_xgb(xgb_model, test[cat_cols+cols]) if xgb_model else np.full_like(y_te, np.nan)\n",
    "except Exception:\n",
    "    yhat_xgb = np.full_like(y_te, np.nan)\n",
    "\n",
    "try:\n",
    "    cb = fit_catboost(train, y_tr, cat_cols, cols)\n",
    "    yhat_cb = cb.predict(test[cat_cols+cols]) if cb else np.full_like(y_te, np.nan)\n",
    "except Exception:\n",
    "    yhat_cb = np.full_like(y_te, np.nan)\n",
    "\n",
    "print(\"GLM metrics:\", metrics_log(y_te, yhat_glm))\n",
    "if not np.isnan(yhat_xgb).all():\n",
    "    print(\"XGB metrics:\", metrics_log(y_te, yhat_xgb))\n",
    "if not np.isnan(yhat_cb).all():\n",
    "    print(\"CatBoost metrics:\", metrics_log(y_te, yhat_cb))\n",
    "\"\"\"))\n",
    "\n",
    "nb_path = \"FishBase_AI_Biology_Pipeline_HF_patched.ipynb\"\n",
    "with open(nb_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "print(\"Patched notebook saved:\", nb_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b5590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cswae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
