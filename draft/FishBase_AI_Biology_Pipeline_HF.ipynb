{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2223a794",
   "metadata": {},
   "source": [
    "\n",
    "# FishBase × AI+Biology (HuggingFace Edition)\n",
    "**Dataset:** [`cboettig/fishbase`](https://huggingface.co/datasets/cboettig/fishbase)  •  **Focus:** Growth (K, L∞) & Natural Mortality (M) prediction  •  **Benchmarks:** Pauly (1980), Hoenig/Hewitt (2005), Then et al. (2015)\n",
    "\n",
    "This notebook provides a **reproducible pipeline**:\n",
    "1) Load FishBase tables (`species`, `ecology`, `popgrowth`, `popchar`) from **Hugging Face** (Parquet)  \n",
    "2) Build a **feature store** with biologically-meaningful variables  \n",
    "3) Train ML models (**GLM / XGBoost / CatBoost / Tabular DL-ready**)  \n",
    "4) Benchmark vs classic estimators **Pauly/Hoenig/Then** with **GroupKFold by Family**\n",
    "\n",
    "> ⚠️ Tip: The dataset repo is **versioned** (e.g., `v24.07`). Pin a specific folder or a commit `revision` for perfect reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d64a2",
   "metadata": {},
   "source": [
    "## 0) Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc373d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment to install dependencies (run-time environment dependent)\n",
    "# %pip install -q pandas numpy scikit-learn xgboost catboost duckdb huggingface_hub pyarrow\n",
    "\n",
    "import os, math, json, itertools, typing as T, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Optional (will skip gracefully if unavailable)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "\n",
    "# Hugging Face config\n",
    "REPO_ID  = \"cboettig/fishbase\"\n",
    "FB_VER   = \"v24.07\"  # pin a version folder within the repo (e.g., v24.07)\n",
    "PARQUET_BASE = f\"data/fb/{FB_VER}/parquet\"\n",
    "\n",
    "DATA_DIR = Path(\"data_fb_ai_bio\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac7512",
   "metadata": {},
   "source": [
    "## 1) Load FishBase tables from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d453a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF Hub] Fallback or error: 401 Client Error. (Request ID: Root=1-68bc5176-7796419154a26b1c146866c5;8e5e98a5-b7ae-4d8a-84ec-d04a1a040c7b)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/cboettig/fishbase/resolve/main/data/fb/v24.07/parquet/species.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "[HF Hub] Fallback or error: 401 Client Error. (Request ID: Root=1-68bc5177-229321a90741c703734ff5ca;1588dfd3-d22a-421c-afde-e416d582e954)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/cboettig/fishbase/resolve/main/data/fb/v24.07/parquet/ecology.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "[HF Hub] Fallback or error: 401 Client Error. (Request ID: Root=1-68bc5178-3689c84e3bac548212c79d47;be4e8d17-004c-41e0-82b0-b1e5d83475a9)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/cboettig/fishbase/resolve/main/data/fb/v24.07/parquet/popgrowth.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "[HF Hub] Fallback or error: 401 Client Error. (Request ID: Root=1-68bc5179-736349a24fe60d300d485304;fae80acb-8231-4c29-8ac0-a54ac43e21f3)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/cboettig/fishbase/resolve/main/data/fb/v24.07/parquet/popchar.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   SpecCode              Genus       Species  SpeciesRefNo  \\\n",
       " 0     64588  Aapticheilichthys      websteri         78622   \n",
       " 1     16239          Aaptosyax        grypus         10431   \n",
       " 2      2347      Abactochromis      labrosus         85491   \n",
       " 3     62612          Abalistes  filamentosus         54835   \n",
       " 4         9          Abalistes     stellatus          9770   \n",
       " \n",
       "                      Author              FBname PicPreferredName  \\\n",
       " 0             (Huber, 2007)                None     Prweb_u0.jpg   \n",
       " 1            Rainboth, 1991   Giant salmon carp     Aagry_u0.gif   \n",
       " 2          (Trewavas, 1935)                None     Melab_u0.jpg   \n",
       " 3  Matsuura & Yoshino, 2004                None     Abfil_u0.jpg   \n",
       " 4         (Anonymous, 1798)  Starry triggerfish     Abste_u4.jpg   \n",
       " \n",
       "   PicPreferredNameM PicPreferredNameF PicPreferredNameJ  ...  Profile   PD50  \\\n",
       " 0              None              None              None  ...     None  1.000   \n",
       " 1              None              None              None  ...     None  1.000   \n",
       " 2              None              None              None  ...     None  1.000   \n",
       " 3              None              None              None  ...     None  0.625   \n",
       " 4              None              None      Abste_u2.jpg  ...     None  0.625   \n",
       " \n",
       "    Emblematic  Entered DateEntered Modified        DateModified Expert  \\\n",
       " 0           0      303  2008-08-04        2 2024-03-21 00:00:00   <NA>   \n",
       " 1           0      113  1996-01-12        2 2024-05-22 00:00:00   <NA>   \n",
       " 2           0        3  1991-03-01      675 2011-12-06 17:06:31     14   \n",
       " 3           0       14  2006-04-19     <NA> 2010-09-08 00:00:00   <NA>   \n",
       " 4           0        2  1990-10-17        2 2024-01-17 00:00:00    124   \n",
       " \n",
       "    DateChecked  TS  \n",
       " 0          NaT NaT  \n",
       " 1          NaT NaT  \n",
       " 2   2010-11-09 NaT  \n",
       " 3          NaT NaT  \n",
       " 4   1996-03-14 NaT  \n",
       " \n",
       " [5 rows x 102 columns],\n",
       "    autoctr  SpecCode  StockCode  EcologyRefNo  HabitatsRef  Neritic  \\\n",
       " 0       77         2          1             3           87        0   \n",
       " 1       78         3          3             3            3        0   \n",
       " 2       79         4          4           189          189       -1   \n",
       " 3       80         5          5            25           25       -1   \n",
       " 4       81         7          9            26           26        0   \n",
       " \n",
       "    SupraLittoralZone  Saltmarshes  LittoralZone  TidePools  ...  \\\n",
       " 0                  0            0             0          0  ...   \n",
       " 1                  0            0             0          0  ...   \n",
       " 2                  0            0             0          0  ...   \n",
       " 3                  0            0             0          0  ...   \n",
       " 4                  0            0             0          0  ...   \n",
       " \n",
       "    RemarksCircadian  CircadianRef  CircadianAlsoRef  Entered  Dateentered  \\\n",
       " 0              None          <NA>              <NA>        4   1990-10-19   \n",
       " 1              None          <NA>              <NA>        4   1990-10-19   \n",
       " 2              None          <NA>              <NA>        1   1990-10-19   \n",
       " 3              None          <NA>              <NA>        2   1990-11-27   \n",
       " 4              None          <NA>              <NA>        3   1990-10-19   \n",
       " \n",
       "    Modified  Datemodified  Expert  Datechecked  TS  \n",
       " 0       309    2011-03-24    <NA>          NaT NaT  \n",
       " 1        97    2015-09-22     711   2001-05-10 NaT  \n",
       " 2       309    2004-09-28    <NA>          NaT NaT  \n",
       " 3       309    2004-09-28    <NA>          NaT NaT  \n",
       " 4       303    2018-11-25    <NA>          NaT NaT  \n",
       " \n",
       " [5 rows x 142 columns],\n",
       "    AutoCtr  StockCode  SpecCode  E_CODE  PopGrowthRef     Sex  \\\n",
       " 0        1          1         2    <NA>           102  female   \n",
       " 1        2          1         2    <NA>           102  female   \n",
       " 2        3          1         2    <NA>           102  female   \n",
       " 3        4          1         2    <NA>           102  female   \n",
       " 4        5          1         2    <NA>           102  female   \n",
       " \n",
       "                  Data  DataSourceRef   Loo  Number  ...  Rm  Comment  \\\n",
       " 0  direct observation            331  15.6    <NA>  ... NaN     None   \n",
       " 1  direct observation            322  19.0    <NA>  ... NaN     None   \n",
       " 2  direct observation            322  22.5    <NA>  ... NaN     None   \n",
       " 3  direct observation            331  24.9    <NA>  ... NaN     None   \n",
       " 4  direct observation            322  40.8    <NA>  ... NaN     None   \n",
       " \n",
       "    Comment2  Entered  DateEntered Modified  DateModified  Expert  DateChecked  \\\n",
       " 0      None        2   1991-05-10        3    2013-02-13    <NA>          NaT   \n",
       " 1      None        2   1991-05-10        3    2013-02-13    <NA>          NaT   \n",
       " 2      None        2   1991-05-10        3    2013-02-13    <NA>          NaT   \n",
       " 3      None        2   1991-05-10        3    2013-02-13    <NA>          NaT   \n",
       " 4      None        2   1991-05-10        3    2013-02-13    <NA>          NaT   \n",
       " \n",
       "    TS  \n",
       " 0 NaT  \n",
       " 1 NaT  \n",
       " 2 NaT  \n",
       " 3 NaT  \n",
       " 4 NaT  \n",
       " \n",
       " [5 rows x 79 columns],\n",
       "    Autoctr  Speccode  Stockcode  PopCharRefNo      Sex  SourceRef    Wmax  \\\n",
       " 0       34         2          1             2  unsexed       <NA>     NaN   \n",
       " 1       35         2          1            81  unsexed       <NA>     NaN   \n",
       " 2       36         2          1            87  unsexed       <NA>     NaN   \n",
       " 3       37         2          1         27445     male      27598     NaN   \n",
       " 4       38         2          1         31148  unsexed      31148  2000.0   \n",
       " \n",
       "      TypeWeight  Lmax  Type  ...  Entered  DateEntered  Modified DateModified  \\\n",
       " 0          None  64.0    TL  ...        1   1991-05-08         1   1997-02-16   \n",
       " 1          None  40.0    TL  ...        1   1991-05-08         1   1991-05-08   \n",
       " 2          None  52.0    TL  ...        1   1991-05-08         1   1991-05-08   \n",
       " 3          None  53.0    TL  ...        3   1998-11-11      <NA>   1998-11-11   \n",
       " 4  total weight   NaN  None  ...        3   1998-08-08      <NA>   1998-08-08   \n",
       " \n",
       "   Expert  DateChecked Comments  SameWL SameLt  TS  \n",
       " 0   <NA>          NaT     None       0      0 NaT  \n",
       " 1   <NA>          NaT     None       0      0 NaT  \n",
       " 2   <NA>          NaT     None       0      0 NaT  \n",
       " 3   <NA>          NaT     None       0      0 NaT  \n",
       " 4   <NA>          NaT     None       0      0 NaT  \n",
       " \n",
       " [5 rows x 25 columns])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_parquet_hf(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a parquet file from Hugging Face repo using huggingface_hub.\n",
    "    If huggingface_hub is unavailable, returns None.\"\"\"\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        fp = hf_hub_download(repo_id=REPO_ID, filename=path)\n",
    "        return pd.read_parquet(fp)\n",
    "    except Exception as e:\n",
    "        print(\"[HF Hub] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_parquet_httpfs(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a parquet file directly via HTTP using DuckDB HTTPFS.\"\"\"\n",
    "    try:\n",
    "        import duckdb\n",
    "        duckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        url = f\"https://huggingface.co/datasets/{REPO_ID}/resolve/main/{path}?download=true\"\n",
    "        return duckdb.sql(f\"\"\"SELECT * FROM read_parquet('{url}')\"\"\").df()\n",
    "    except Exception as e:\n",
    "        print(\"[DuckDB HTTPFS] Fallback or error:\", e)\n",
    "        return None\n",
    "\n",
    "def load_fb_table(name: str) -> pd.DataFrame:\n",
    "    rel = f\"{PARQUET_BASE}/{name}.parquet\"\n",
    "    df = load_parquet_hf(rel)\n",
    "    if df is None:\n",
    "        df = load_parquet_httpfs(rel)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Cannot load {name} via HF or HTTPFS. Please check connectivity or install deps.\")\n",
    "    return df\n",
    "\n",
    "species  = load_fb_table(\"species\")\n",
    "ecology  = load_fb_table(\"ecology\")\n",
    "popgrowth= load_fb_table(\"popgrowth\")\n",
    "popchar  = load_fb_table(\"popchar\")\n",
    "\n",
    "species.head(), ecology.head(), popgrowth.head(), popchar.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae7b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q duckdb-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8bc83",
   "metadata": {},
   "source": [
    "## 2) Schema checks & column aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131b78b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species': ['SpecCode',\n",
       "  'Genus',\n",
       "  'Species',\n",
       "  'SpeciesRefNo',\n",
       "  'Author',\n",
       "  'FBname',\n",
       "  'PicPreferredName',\n",
       "  'PicPreferredNameM',\n",
       "  'PicPreferredNameF',\n",
       "  'PicPreferredNameJ',\n",
       "  'FamCode',\n",
       "  'Subfamily',\n",
       "  'GenCode',\n",
       "  'SubGenCode',\n",
       "  'BodyShapeI',\n",
       "  'Source',\n",
       "  'AuthorRef',\n",
       "  'Remark',\n",
       "  'TaxIssue',\n",
       "  'Fresh',\n",
       "  'Brack',\n",
       "  'Saltwater',\n",
       "  'DemersPelag',\n",
       "  'AirBreathing',\n",
       "  'AirBreathingRef',\n",
       "  'AnaCat',\n",
       "  'MigratRef',\n",
       "  'DepthRangeShallow',\n",
       "  'DepthRangeDeep',\n",
       "  'DepthRangeRef',\n",
       "  'DepthRangeComShallow',\n",
       "  'DepthRangeComDeep',\n",
       "  'DepthComRef',\n",
       "  'LongevityWild',\n",
       "  'LongevityWildRef',\n",
       "  'LongevityCaptive',\n",
       "  'LongevityCapRef',\n",
       "  'Vulnerability',\n",
       "  'VulnerabilityClimate',\n",
       "  'Length',\n",
       "  'LTypeMaxM',\n",
       "  'LengthFemale',\n",
       "  'LTypeMaxF',\n",
       "  'MaxLengthRef',\n",
       "  'CommonLength',\n",
       "  'LTypeComM',\n",
       "  'CommonLengthF',\n",
       "  'LTypeComF',\n",
       "  'CommonLengthRef',\n",
       "  'Weight',\n",
       "  'WeightFemale',\n",
       "  'MaxWeightRef',\n",
       "  'Pic',\n",
       "  'PictureFemale',\n",
       "  'LarvaPic',\n",
       "  'EggPic',\n",
       "  'ImportanceRef',\n",
       "  'Importance',\n",
       "  'PriceCateg',\n",
       "  'PriceReliability',\n",
       "  'Remarks7',\n",
       "  'LandingStatistics',\n",
       "  'Landings',\n",
       "  'MainCatchingMethod',\n",
       "  'II',\n",
       "  'MSeines',\n",
       "  'MGillnets',\n",
       "  'MCastnets',\n",
       "  'MTraps',\n",
       "  'MSpears',\n",
       "  'MTrawls',\n",
       "  'MDredges',\n",
       "  'MLiftnets',\n",
       "  'MHooksLines',\n",
       "  'MOther',\n",
       "  'UsedforAquaculture',\n",
       "  'LifeCycle',\n",
       "  'AquacultureRef',\n",
       "  'UsedasBait',\n",
       "  'BaitRef',\n",
       "  'Aquarium',\n",
       "  'AquariumFishII',\n",
       "  'AquariumRef',\n",
       "  'GameFish',\n",
       "  'GameRef',\n",
       "  'Dangerous',\n",
       "  'DangerousRef',\n",
       "  'Electrogenic',\n",
       "  'ElectroRef',\n",
       "  'Complete',\n",
       "  'GoogleImage',\n",
       "  'Comments',\n",
       "  'Profile',\n",
       "  'PD50',\n",
       "  'Emblematic',\n",
       "  'Entered',\n",
       "  'DateEntered',\n",
       "  'Modified',\n",
       "  'DateModified',\n",
       "  'Expert',\n",
       "  'DateChecked',\n",
       "  'TS'],\n",
       " 'ecology': ['autoctr',\n",
       "  'SpecCode',\n",
       "  'StockCode',\n",
       "  'EcologyRefNo',\n",
       "  'HabitatsRef',\n",
       "  'Neritic',\n",
       "  'SupraLittoralZone',\n",
       "  'Saltmarshes',\n",
       "  'LittoralZone',\n",
       "  'TidePools',\n",
       "  'Intertidal',\n",
       "  'SubLittoral',\n",
       "  'Caves',\n",
       "  'Oceanic',\n",
       "  'Epipelagic',\n",
       "  'Mesopelagic',\n",
       "  'Bathypelagic',\n",
       "  'Abyssopelagic',\n",
       "  'Hadopelagic',\n",
       "  'Estuaries',\n",
       "  'Mangroves',\n",
       "  'MarshesSwamps',\n",
       "  'CaveAnchialine',\n",
       "  'Stream',\n",
       "  'Lakes',\n",
       "  'Cave',\n",
       "  'Cave2',\n",
       "  'Herbivory2',\n",
       "  'HerbivoryRef',\n",
       "  'FeedingType',\n",
       "  'FeedingTypeRef',\n",
       "  'DietTroph',\n",
       "  'DietSeTroph',\n",
       "  'DietTLu',\n",
       "  'DietseTLu',\n",
       "  'DietRemark',\n",
       "  'DietRef',\n",
       "  'FoodTroph',\n",
       "  'FoodSeTroph',\n",
       "  'FoodRemark',\n",
       "  'FoodRef',\n",
       "  'AddRems',\n",
       "  'AssociationRef',\n",
       "  'Parasitism',\n",
       "  'Solitary',\n",
       "  'Symbiosis',\n",
       "  'Symphorism',\n",
       "  'Commensalism',\n",
       "  'Mutualism',\n",
       "  'Epiphytic',\n",
       "  'Schooling',\n",
       "  'SchoolingFrequency',\n",
       "  'SchoolingLifestage',\n",
       "  'Shoaling',\n",
       "  'ShoalingFrequency',\n",
       "  'ShoalingLifestage',\n",
       "  'SchoolShoalRef',\n",
       "  'AssociationsWith',\n",
       "  'AssociationsRemarks',\n",
       "  'OutsideHost',\n",
       "  'OHRemarks',\n",
       "  'InsideHost',\n",
       "  'IHRemarks',\n",
       "  'SubstrateRef',\n",
       "  'Benthic',\n",
       "  'Sessile',\n",
       "  'Mobile',\n",
       "  'Demersal',\n",
       "  'Endofauna',\n",
       "  'Pelagic',\n",
       "  'Megabenthos',\n",
       "  'Macrobenthos',\n",
       "  'Meiobenthos',\n",
       "  'SoftBottom',\n",
       "  'Sand',\n",
       "  'Coarse',\n",
       "  'Fine',\n",
       "  'Level',\n",
       "  'Sloping',\n",
       "  'Silt',\n",
       "  'Mud',\n",
       "  'Ooze',\n",
       "  'Detritus',\n",
       "  'Organic',\n",
       "  'HardBottom',\n",
       "  'Rocky',\n",
       "  'Rubble',\n",
       "  'Gravel',\n",
       "  'SpecialHabitatRef',\n",
       "  'Macrophyte',\n",
       "  'BedsBivalve',\n",
       "  'BedsRock',\n",
       "  'SeaGrassBeds',\n",
       "  'BedsOthers',\n",
       "  'CoralReefs',\n",
       "  'ReefExclusive',\n",
       "  'DropOffs',\n",
       "  'ReefFlats',\n",
       "  'Lagoons',\n",
       "  'Burrows',\n",
       "  'Tunnels',\n",
       "  'Guyots',\n",
       "  'Crevices',\n",
       "  'Seamounts',\n",
       "  'ColdSeeps',\n",
       "  'HydrothermalVents',\n",
       "  'DeepWaterCorals',\n",
       "  'Vegetation',\n",
       "  'Leaves',\n",
       "  'Stems',\n",
       "  'Roots',\n",
       "  'Driftwood',\n",
       "  'OInverterbrates',\n",
       "  'OIRemarks',\n",
       "  'Verterbrates',\n",
       "  'VRemarks',\n",
       "  'Pilings',\n",
       "  'RicePaddies',\n",
       "  'BoatHulls',\n",
       "  'Corals',\n",
       "  'SoftCorals',\n",
       "  'OnPolyp',\n",
       "  'BetweenPolyps',\n",
       "  'HardCorals',\n",
       "  'OnExoskeleton',\n",
       "  'InterstitialSpaces',\n",
       "  'Circadian1',\n",
       "  'Circadian2',\n",
       "  'Circadian3',\n",
       "  'BioAspect1',\n",
       "  'BioAspect2',\n",
       "  'BioAspect3',\n",
       "  'RemarksCircadian',\n",
       "  'CircadianRef',\n",
       "  'CircadianAlsoRef',\n",
       "  'Entered',\n",
       "  'Dateentered',\n",
       "  'Modified',\n",
       "  'Datemodified',\n",
       "  'Expert',\n",
       "  'Datechecked',\n",
       "  'TS'],\n",
       " 'popgrowth': ['AutoCtr',\n",
       "  'StockCode',\n",
       "  'SpecCode',\n",
       "  'E_CODE',\n",
       "  'PopGrowthRef',\n",
       "  'Sex',\n",
       "  'Data',\n",
       "  'DataSourceRef',\n",
       "  'Loo',\n",
       "  'Number',\n",
       "  'r2',\n",
       "  'SE_Loo',\n",
       "  'SD_Loo',\n",
       "  'LCL_Loo',\n",
       "  'UCL_Loo',\n",
       "  'AssumedDistLoo',\n",
       "  'TLinfinity',\n",
       "  'K',\n",
       "  'SE_K',\n",
       "  'SD_K',\n",
       "  'LCL_K',\n",
       "  'UCL_K',\n",
       "  'AssumedDistK',\n",
       "  'to',\n",
       "  'SE_to',\n",
       "  'SD_to',\n",
       "  'LCL_to',\n",
       "  'UCL_to',\n",
       "  'Type',\n",
       "  'MethodGrowth',\n",
       "  'Winfinity',\n",
       "  'LinfLmax',\n",
       "  'Auxim',\n",
       "  'LogKLogLoo',\n",
       "  'SourceWinfinity',\n",
       "  'b',\n",
       "  'C',\n",
       "  'tmax',\n",
       "  'tmaxRef',\n",
       "  'tm',\n",
       "  'M',\n",
       "  'MethodM',\n",
       "  'Mquality',\n",
       "  'MRef',\n",
       "  'Number_M',\n",
       "  'r2_M',\n",
       "  'SE_M',\n",
       "  'SD_M',\n",
       "  'LCL_M',\n",
       "  'UCL_M',\n",
       "  'AssumedDistM',\n",
       "  'Lm',\n",
       "  'LmLoo',\n",
       "  'LmSex',\n",
       "  'TypeLm',\n",
       "  'unsexedRef',\n",
       "  'LmMale',\n",
       "  'LmLooMale',\n",
       "  'LmFemale',\n",
       "  'LmLooFemale',\n",
       "  'Locality',\n",
       "  'YearStart',\n",
       "  'YearEnd',\n",
       "  'YearRemark',\n",
       "  'C_Code',\n",
       "  'GrowthEnviron',\n",
       "  'Temperature',\n",
       "  'DeltaT',\n",
       "  'TempRef',\n",
       "  'Rm',\n",
       "  'Comment',\n",
       "  'Comment2',\n",
       "  'Entered',\n",
       "  'DateEntered',\n",
       "  'Modified',\n",
       "  'DateModified',\n",
       "  'Expert',\n",
       "  'DateChecked',\n",
       "  'TS'],\n",
       " 'popchar': ['Autoctr',\n",
       "  'Speccode',\n",
       "  'Stockcode',\n",
       "  'PopCharRefNo',\n",
       "  'Sex',\n",
       "  'SourceRef',\n",
       "  'Wmax',\n",
       "  'TypeWeight',\n",
       "  'Lmax',\n",
       "  'Type',\n",
       "  'LmaxQuality',\n",
       "  'tmax',\n",
       "  'tmaxQuality',\n",
       "  'Locality',\n",
       "  'C_Code',\n",
       "  'Entered',\n",
       "  'DateEntered',\n",
       "  'Modified',\n",
       "  'DateModified',\n",
       "  'Expert',\n",
       "  'DateChecked',\n",
       "  'Comments',\n",
       "  'SameWL',\n",
       "  'SameLt',\n",
       "  'TS']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def first_col(df, candidates: T.List[str], required=False):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of {candidates} found in columns: {list(df.columns)[:20]}...\")\n",
    "    return None\n",
    "\n",
    "# Aliases to handle minor schema differences across FishBase snapshots\n",
    "ALIASES = {\n",
    "    \"SpecCode\": [\"SpecCode\", \"SpecCode_x\", \"SpecCode_y\"],\n",
    "    \"Family\": [\"Family\"],\n",
    "    \"Order\": [\"Order\"],\n",
    "    \"Class\": [\"Class\"],\n",
    "    \"BodyShapeI\": [\"BodyShapeI\", \"BodyShape\"],\n",
    "    \"DemersPelag\": [\"DemersPelag\", \"DemersPelagics\"],\n",
    "    \"AnaCat\": [\"AnaCat\"],\n",
    "    \"EnvTemp\": [\"EnvTemp\"],\n",
    "    \"DepthRangeShallow\": [\"DepthRangeShallow\", \"DepthShallow\"],\n",
    "    \"DepthRangeDeep\": [\"DepthRangeDeep\", \"DepthDeep\"],\n",
    "    \"Fresh\": [\"Fresh\"],\n",
    "    \"Brack\": [\"Brack\"],\n",
    "    \"Saltwater\": [\"Saltwater\"],\n",
    "    \"Length\": [\"Length\"],\n",
    "    \"LTypeMaxM\": [\"LTypeMaxM\"],\n",
    "    \"Weight\": [\"Weight\"],\n",
    "    \"LongevityWild\": [\"LongevityWild\", \"Longevity\"],\n",
    "\n",
    "    # Ecology\n",
    "    \"FoodTroph\": [\"FoodTroph\"],\n",
    "    \"DietTroph\": [\"DietTroph\"],\n",
    "    \"FoodSeTroph\": [\"FoodSeTroph\"],\n",
    "    \"DietSeTroph\": [\"DietSeTroph\"],\n",
    "\n",
    "    # Popgrowth\n",
    "    \"Loo\": [\"Loo\", \"Linf\", \"Linf_cm\", \"Linf_cm_\", \"Linfinity\"],\n",
    "    \"K\": [\"K\"],\n",
    "    \"to\": [\"to\", \"t0\"],\n",
    "    \"M\": [\"M\"],\n",
    "    \"tm\": [\"tm\", \"tm50\"],\n",
    "    \"Lm\": [\"Lm\", \"Lm50\"],\n",
    "\n",
    "    # Popchar\n",
    "    \"tmax\": [\"tmax\",\"Tmax\",\"LongevityWild\"],  # fallback if tmax absent\n",
    "    \"Lmax\": [\"Lmax\"],\n",
    "    \"Wmax\": [\"Wmax\"],\n",
    "}\n",
    "\n",
    "def alias(df: pd.DataFrame, key: str, required=False):\n",
    "    return first_col(df, ALIASES[key], required=required)\n",
    "\n",
    "# Keep a copy of original column names for reference\n",
    "orig_cols = {\n",
    "    \"species\": list(species.columns),\n",
    "    \"ecology\": list(ecology.columns),\n",
    "    \"popgrowth\": list(popgrowth.columns),\n",
    "    \"popchar\": list(popchar.columns),\n",
    "}\n",
    "orig_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bd520",
   "metadata": {},
   "source": [
    "## 3) Feature engineering & targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e71c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 'SpecCode' missing in popchar -> skip merge; using LongevityWild as fallback.\n",
      "Feature store saved: data_fb_ai_bio\\feature_store.parquet rows: 47203\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecCode</th>\n",
       "      <th>BodyShapeI</th>\n",
       "      <th>DemersPelag</th>\n",
       "      <th>AnaCat</th>\n",
       "      <th>DepthRangeShallow</th>\n",
       "      <th>DepthRangeDeep</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Brack</th>\n",
       "      <th>Saltwater</th>\n",
       "      <th>Length</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>M</th>\n",
       "      <th>tm</th>\n",
       "      <th>Lm</th>\n",
       "      <th>tmax</th>\n",
       "      <th>T_proxy</th>\n",
       "      <th>log_K</th>\n",
       "      <th>log_Loo</th>\n",
       "      <th>log_M</th>\n",
       "      <th>tmax_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64588</td>\n",
       "      <td>fusiform / normal</td>\n",
       "      <td>benthopelagic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16239</td>\n",
       "      <td>fusiform / normal</td>\n",
       "      <td>pelagic</td>\n",
       "      <td>potamodromous</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2347</td>\n",
       "      <td>short and / or deep</td>\n",
       "      <td>demersal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62612</td>\n",
       "      <td>short and / or deep</td>\n",
       "      <td>pelagic-neritic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>short and / or deep</td>\n",
       "      <td>demersal</td>\n",
       "      <td>amphidromous</td>\n",
       "      <td>7</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SpecCode           BodyShapeI      DemersPelag         AnaCat  \\\n",
       "0     64588    fusiform / normal    benthopelagic            NaN   \n",
       "1     16239    fusiform / normal          pelagic  potamodromous   \n",
       "2      2347  short and / or deep         demersal            NaN   \n",
       "3     62612  short and / or deep  pelagic-neritic            NaN   \n",
       "4         9  short and / or deep         demersal   amphidromous   \n",
       "\n",
       "   DepthRangeShallow  DepthRangeDeep  Fresh  Brack  Saltwater  Length  ...  \\\n",
       "0               <NA>            <NA>      1      0          0    3.08  ...   \n",
       "1               <NA>            <NA>      1      0          0  130.00  ...   \n",
       "2                  0              30      1      0          0   11.50  ...   \n",
       "3                 61             180      0      0          1   32.50  ...   \n",
       "4                  7             350      0      0          1   60.00  ...   \n",
       "\n",
       "   to   M  tm  Lm  tmax  T_proxy  log_K  log_Loo  log_M  tmax_any  \n",
       "0 NaN NaN NaN NaN   NaN      NaN    NaN      NaN    NaN       NaN  \n",
       "1 NaN NaN NaN NaN   NaN      NaN    NaN      NaN    NaN       NaN  \n",
       "2 NaN NaN NaN NaN   NaN      NaN    NaN      NaN    NaN       NaN  \n",
       "3 NaN NaN NaN NaN   NaN      NaN    NaN      NaN    NaN       NaN  \n",
       "4 NaN NaN NaN NaN   NaN      NaN    NaN      NaN    NaN       NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def map_envtemp_to_T(envtemp: str) -> float:\n",
    "    m = {\"Tropical\":27.0, \"Subtropical\":20.0, \"Temperate\":12.0, \"Polar\":2.0}\n",
    "    if pd.isna(envtemp):\n",
    "        return np.nan\n",
    "    return m.get(str(envtemp).strip().title(), np.nan)\n",
    "\n",
    "# Coerce key dtypes & select columns by aliases\n",
    "def coerce_numeric(df: pd.DataFrame, cols: T.List[str]):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# Build unified feature table\n",
    "def build_feature_store(species, ecology, popgrowth, popchar) -> pd.DataFrame:\n",
    "    sp = species.copy()\n",
    "    ec = ecology.copy()\n",
    "    pg = popgrowth.copy()\n",
    "    pc = popchar.copy()\n",
    "\n",
    "    # Coerce IDs\n",
    "    sc = alias(sp, \"SpecCode\", required=True)\n",
    "    for df in (sp, ec, pg, pc):\n",
    "        if alias(df, \"SpecCode\") is None:\n",
    "            continue\n",
    "        df[alias(df, \"SpecCode\")] = pd.to_numeric(df[alias(df, \"SpecCode\")], errors=\"coerce\")\n",
    "\n",
    "    # Reduce to needed columns (by alias lookup)\n",
    "    sp_cols = [x for k in [\"SpecCode\",\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\n",
    "                           \"DepthRangeShallow\",\"DepthRangeDeep\",\"Fresh\",\"Brack\",\"Saltwater\",\"Length\",\"LTypeMaxM\",\n",
    "                           \"Weight\",\"LongevityWild\"] if (x:=alias(sp,k))] + []\n",
    "    ec_cols = [x for k in [\"SpecCode\",\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"] if (x:=alias(ec,k))]\n",
    "    pg_cols = [x for k in [\"SpecCode\",\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"] if (x:=alias(pg,k))]\n",
    "    pc_cols = [x for k in [\"SpecCode\",\"tmax\",\"Lmax\",\"Wmax\"] if (x:=alias(pc,k))]\n",
    "\n",
    "    sp1 = sp[sp_cols].copy()\n",
    "    ec1 = ec[ec_cols].copy() if ec_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pg1 = pg[pg_cols].copy() if pg_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "    pc1 = pc[pc_cols].copy() if pc_cols else pd.DataFrame(columns=[\"SpecCode\"])\n",
    "\n",
    "    # Standardize names\n",
    "    def stdcol(df, key):\n",
    "        c = alias(df, key)\n",
    "        if c and c != key:\n",
    "            df = df.rename(columns={c:key})\n",
    "        return df\n",
    "\n",
    "    for key in [\"SpecCode\",\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\n",
    "                \"DepthRangeShallow\",\"DepthRangeDeep\",\"Fresh\",\"Brack\",\"Saltwater\",\"Length\",\"LTypeMaxM\",\n",
    "                \"Weight\",\"LongevityWild\"]:\n",
    "        sp1 = stdcol(sp1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"]:\n",
    "        ec1 = stdcol(ec1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"]:\n",
    "        pg1 = stdcol(pg1, key)\n",
    "\n",
    "    for key in [\"SpecCode\",\"tmax\",\"Lmax\",\"Wmax\"]:\n",
    "        pc1 = stdcol(pc1, key)\n",
    "\n",
    "    # Numeric coercion\n",
    "    sp1 = coerce_numeric(sp1, [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"LongevityWild\",\n",
    "                               \"Fresh\",\"Brack\",\"Saltwater\"])\n",
    "    ec1 = coerce_numeric(ec1, [\"FoodTroph\",\"FoodSeTroph\",\"DietTroph\",\"DietSeTroph\"])\n",
    "    pg1 = coerce_numeric(pg1, [\"Loo\",\"K\",\"to\",\"M\",\"tm\",\"Lm\"])\n",
    "    pc1 = coerce_numeric(pc1, [\"tmax\",\"Lmax\",\"Wmax\"])\n",
    "\n",
    "    # Merge tables on SpecCode\n",
    "    df = sp1.copy()\n",
    "\n",
    "    if \"SpecCode\" in ec1.columns:\n",
    "        df = df.merge(ec1, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in ecology -> skip merge.\")\n",
    "\n",
    "    if \"SpecCode\" in pg1.columns:\n",
    "        df = df.merge(pg1, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in popgrowth -> skip merge.\")\n",
    "\n",
    "    if \"SpecCode\" in pc1.columns:\n",
    "        df = df.merge(pc1, on=\"SpecCode\", how=\"left\")\n",
    "    else:\n",
    "        print(\"[WARN] 'SpecCode' missing in popchar -> skip merge; using LongevityWild as fallback.\")\n",
    "        df[\"tmax\"] = np.nan\n",
    "\n",
    "    # Derived features\n",
    "    df[\"T_proxy\"] = df.get(\"EnvTemp\", pd.Series(index=df.index)).map(map_envtemp_to_T)\n",
    "\n",
    "    # Targets (log-scale)\n",
    "    for t in [\"K\",\"Loo\",\"M\"]:\n",
    "        if t in df.columns:\n",
    "            df[f\"log_{t}\"] = np.log(df[t].astype(float))\n",
    "\n",
    "    # tmax_any: prefer tmax, fallback LongevityWild\n",
    "    if \"tmax\" in df.columns:\n",
    "        df[\"tmax_any\"] = df[\"tmax\"].where(~df[\"tmax\"].isna(), df.get(\"LongevityWild\", np.nan))\n",
    "    else:\n",
    "        df[\"tmax_any\"] = df.get(\"LongevityWild\", np.nan)\n",
    "\n",
    "    # Categoricals\n",
    "    for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    out_path = DATA_DIR / \"feature_store.parquet\"\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(\"Feature store saved:\", out_path, \"rows:\", len(df))\n",
    "    return df\n",
    "\n",
    "feature_store = build_feature_store(species, ecology, popgrowth, popchar)\n",
    "feature_store.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe33175",
   "metadata": {},
   "source": [
    "## 4) Baseline estimators (Pauly, Hoenig/Hewitt, Then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cbcbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pauly_M(K, Linf_cm, T_celsius):\n",
    "    if any(pd.isna(x) for x in [K, Linf_cm, T_celsius]):\n",
    "        return np.nan\n",
    "    val = -0.0066 - 0.279*math.log10(Linf_cm) + 0.6543*math.log10(K) + 0.4634*math.log10(T_celsius)\n",
    "    return 10 ** val\n",
    "\n",
    "def hoenig_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0:\n",
    "        return np.nan\n",
    "    return 4.22 / tmax  # Hewitt & Hoenig (2005) rule-of-thumb\n",
    "\n",
    "def then_M_from_tmax(tmax):\n",
    "    if pd.isna(tmax) or tmax <= 0:\n",
    "        return np.nan\n",
    "    return 4.899 * (tmax ** -0.916)\n",
    "\n",
    "def then_M_from_growth(K, Linf_cm):\n",
    "    if any(pd.isna(x) or x<=0 for x in [K, Linf_cm]):\n",
    "        return np.nan\n",
    "    return 1.521 * (K ** 0.72) * (Linf_cm ** -0.33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef581c",
   "metadata": {},
   "source": [
    "## 5) Splits & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf749da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_splits(df: pd.DataFrame, group_col_candidates=[\"Family\",\"Order\",\"Class\",\"SpecCode\"], n_splits=5):\n",
    "    available = [c for c in group_col_candidates if c in df.columns]\n",
    "    if not available:\n",
    "        raise KeyError(\"Không tìm thấy bất kỳ cột group nào.\")\n",
    "    group_col = available[0]\n",
    "    print(f\"[INFO] Using group_col = {group_col}\")\n",
    "    \n",
    "    df_ = df.dropna(subset=[group_col]).copy()\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    groups = df_[group_col].astype(str).values\n",
    "    for fold, (tr, te) in enumerate(gkf.split(df_, groups=groups)):\n",
    "        yield fold, df_.iloc[tr].copy(), df_.iloc[te].copy()\n",
    "\n",
    "\n",
    "def metrics_log(y_true_log, y_pred_log):\n",
    "    rmse = math.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2  = r2_score(y_true_log, y_pred_log)\n",
    "    # MAPE on original scale\n",
    "    y_true = np.exp(y_true_log)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))))\n",
    "    return dict(rmse_log=rmse, mae_log=mae, r2=r2, mape=mape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6457a",
   "metadata": {},
   "source": [
    "## 6) Models (GLM / XGBoost / CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4b5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_glm_numeric(X_train, y_train_log, num_cols):\n",
    "    pre = ColumnTransformer([(\"num\", StandardScaler(), num_cols)], remainder=\"drop\")\n",
    "    model = Pipeline([(\"prep\", pre), (\"lin\", LinearRegression())])\n",
    "    model.fit(X_train[num_cols], y_train_log)\n",
    "    return model\n",
    "\n",
    "def fit_xgb(train_df, y_train_log, cat_cols, num_cols):\n",
    "    if xgb is None:\n",
    "        return None\n",
    "    X = pd.get_dummies(train_df[cat_cols+num_cols], drop_first=False)\n",
    "    dtrain = xgb.DMatrix(X, label=y_train_log)\n",
    "    params = dict(objective=\"reg:squarederror\", eval_metric=\"rmse\",\n",
    "                  eta=0.03, max_depth=8, subsample=0.8, colsample_bytree=0.8, seed=RANDOM_SEED)\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=400, verbose_eval=False)\n",
    "    return bst, X.columns.tolist()\n",
    "\n",
    "def predict_xgb(model_tuple, X_df):\n",
    "    bst, cols = model_tuple\n",
    "    X = pd.get_dummies(X_df, drop_first=False)\n",
    "    for c in cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    X = X[cols]\n",
    "    dtest = xgb.DMatrix(X)\n",
    "    return bst.predict(dtest)\n",
    "\n",
    "def fit_catboost(train_df, y_train_log, cat_cols, num_cols):\n",
    "    if CatBoostRegressor is None:\n",
    "        return None\n",
    "    X = train_df[cat_cols+num_cols].copy()\n",
    "    cat_idx = [X.columns.get_loc(c) for c in cat_cols]\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\", depth=8, learning_rate=0.05, l2_leaf_reg=6, iterations=400,\n",
    "        verbose=False, random_seed=RANDOM_SEED\n",
    "    )\n",
    "    model.fit(X, y_train_log, cat_features=cat_idx)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a280b7",
   "metadata": {},
   "source": [
    "## 7) Experiment **Task B** — Predict `log_M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6758a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using group_col = SpecCode\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m cols \u001b[38;5;241m=\u001b[39m num_base \u001b[38;5;241m+\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_num\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     24\u001b[0m dsub \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequire\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m cols)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, train, test \u001b[38;5;129;01min\u001b[39;00m group_splits(dsub, group_col_candidates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFamily\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecCode\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     27\u001b[0m     y_tr \u001b[38;5;241m=\u001b[39m train[target]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     28\u001b[0m     y_te \u001b[38;5;241m=\u001b[39m test[target]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mgroup_splits\u001b[1;34m(df, group_col_candidates, n_splits)\u001b[0m\n\u001b[0;32m      9\u001b[0m gkf \u001b[38;5;241m=\u001b[39m GroupKFold(n_splits\u001b[38;5;241m=\u001b[39mn_splits)\n\u001b[0;32m     10\u001b[0m groups \u001b[38;5;241m=\u001b[39m df_[group_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (tr, te) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gkf\u001b[38;5;241m.\u001b[39msplit(df_, groups\u001b[38;5;241m=\u001b[39mgroups)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m fold, df_\u001b[38;5;241m.\u001b[39miloc[tr]\u001b[38;5;241m.\u001b[39mcopy(), df_\u001b[38;5;241m.\u001b[39miloc[te]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\cswae\\lib\\site-packages\\sklearn\\model_selection\\_split.py:404\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    402\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    405\u001b[0m         (\n\u001b[0;32m    406\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=0."
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_parquet(DATA_DIR/\"feature_store.parquet\")\n",
    "\n",
    "cat_cols = [c for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"] if c in df.columns]\n",
    "num_base = [c for c in [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"FoodTroph\",\"DietTroph\"] if c in df.columns]\n",
    "\n",
    "results = []\n",
    "task = \"B\"\n",
    "target = \"log_M\"\n",
    "\n",
    "scenarios = {\n",
    "    \"B_full\": {\"extra_num\": [\"K\",\"Loo\",\"T_proxy\"], \"require\": [\"log_M\"]},\n",
    "    \"B_lite\": {\"extra_num\": [], \"require\": [\"log_M\"]}\n",
    "}\n",
    "\n",
    "def compute_baseline_logs(test):\n",
    "    p = test.apply(lambda r: pauly_M(r.get(\"K\"), r.get(\"Loo\"), r.get(\"T_proxy\")), axis=1)\n",
    "    h = test.apply(lambda r: hoenig_M_from_tmax(r.get(\"tmax_any\")), axis=1)\n",
    "    t1= test.apply(lambda r: then_M_from_tmax(r.get(\"tmax_any\")), axis=1)\n",
    "    t2= test.apply(lambda r: then_M_from_growth(r.get(\"K\"), r.get(\"Loo\")), axis=1)\n",
    "    return np.log(p), np.log(h), np.log(t1), np.log(t2)\n",
    "\n",
    "for scen, cfg in scenarios.items():\n",
    "    cols = num_base + [c for c in cfg[\"extra_num\"] if c in df.columns]\n",
    "    dsub = df.dropna(subset=cfg[\"require\"] + cols).copy()\n",
    "\n",
    "    for fold, train, test in group_splits(dsub, group_col_candidates=[\"Family\",\"Order\",\"Class\",\"SpecCode\"], n_splits=5):\n",
    "        y_tr = train[target].values\n",
    "        y_te = test[target].values\n",
    "\n",
    "        # GLM numeric\n",
    "        glm = fit_glm_numeric(train, y_tr, num_cols=cols)\n",
    "        yhat_glm = glm.predict(test[cols])\n",
    "\n",
    "        # XGB\n",
    "        if xgb is not None:\n",
    "            xgb_model = fit_xgb(train, y_tr, cat_cols, cols)\n",
    "            yhat_xgb = predict_xgb(xgb_model, test[cat_cols+cols])\n",
    "        else:\n",
    "            yhat_xgb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        # CatBoost\n",
    "        if CatBoostRegressor is not None:\n",
    "            cb = fit_catboost(train, y_tr, cat_cols, cols)\n",
    "            yhat_cb = cb.predict(test[cat_cols+cols])\n",
    "        else:\n",
    "            yhat_cb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        pauly_log, hoenig_log, then_tmax_log, then_growth_log = compute_baseline_logs(test)\n",
    "\n",
    "        for name, pred in {\"GLM\":yhat_glm, \"XGBoost\":yhat_xgb, \"CatBoost\":yhat_cb}.items():\n",
    "            m = metrics_log(y_te, pred)\n",
    "            # ΔRMSE vs baselines\n",
    "            def rmse_of(b):\n",
    "                mask = ~np.isnan(b)\n",
    "                return math.sqrt(mean_squared_error(y_te[mask], b[mask]))\n",
    "            try: d_pauly = m[\"rmse_log\"] - rmse_of(pauly_log.values if hasattr(pauly_log,\"values\") else pauly_log)\n",
    "            except: d_pauly = np.nan\n",
    "            try: d_hoenig = m[\"rmse_log\"] - rmse_of(hoenig_log.values if hasattr(hoenig_log,\"values\") else hoenig_log)\n",
    "            except: d_hoenig = np.nan\n",
    "            try: d_then1 = m[\"rmse_log\"] - rmse_of(then_tmax_log.values if hasattr(then_tmax_log,\"values\") else then_tmax_log)\n",
    "            except: d_then1 = np.nan\n",
    "            try: d_then2 = m[\"rmse_log\"] - rmse_of(then_growth_log.values if hasattr(then_growth_log,\"values\") else then_growth_log)\n",
    "            except: d_then2 = np.nan\n",
    "\n",
    "            results.append(dict(task=task, scenario=scen, target=target, fold=fold, model=name, **m,\n",
    "                                delta_rmse_vs_pauly=d_pauly, delta_rmse_vs_hoenig=d_hoenig,\n",
    "                                delta_rmse_vs_then_tmax=d_then1, delta_rmse_vs_then_growth=d_then2,\n",
    "                                n_test=int(len(y_te))))\n",
    "\n",
    "resB = pd.DataFrame(results).sort_values([\"scenario\",\"model\",\"fold\"])\n",
    "resB_path = DATA_DIR / \"benchmark_results_M.csv\"\n",
    "resB.to_csv(resB_path, index=False)\n",
    "resB.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e91144",
   "metadata": {},
   "source": [
    "## 8) Experiment **Task A** — Predict `log_K` & `log_Loo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc506169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(DATA_DIR/\"feature_store.parquet\")\n",
    "cat_cols = [c for c in [\"Family\",\"Order\",\"Class\",\"BodyShapeI\",\"DemersPelag\",\"AnaCat\",\"EnvTemp\",\"LTypeMaxM\"] if c in df.columns]\n",
    "num_cols = [c for c in [\"DepthRangeShallow\",\"DepthRangeDeep\",\"Length\",\"Weight\",\"FoodTroph\",\"DietTroph\",\"T_proxy\"] if c in df.columns]\n",
    "\n",
    "targets = [t for t in [\"log_K\",\"log_Loo\"] if t in df.columns]\n",
    "rows = []\n",
    "for target in targets:\n",
    "    dsub = df.dropna(subset=[target]).copy()\n",
    "    for fold, train, test in group_splits(dsub, group_col=\"Family\", n_splits=5):\n",
    "        y_tr = train[target].values\n",
    "        y_te = test[target].values\n",
    "\n",
    "        glm = fit_glm_numeric(train, y_tr, num_cols=num_cols)\n",
    "        yhat_glm = glm.predict(test[num_cols])\n",
    "\n",
    "        if xgb is not None:\n",
    "            xgb_model = fit_xgb(train, y_tr, cat_cols, num_cols)\n",
    "            yhat_xgb = predict_xgb(xgb_model, test[cat_cols+num_cols])\n",
    "        else:\n",
    "            yhat_xgb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        if CatBoostRegressor is not None:\n",
    "            cb = fit_catboost(train, y_tr, cat_cols, num_cols)\n",
    "            yhat_cb = cb.predict(test[cat_cols+num_cols])\n",
    "        else:\n",
    "            yhat_cb = np.full_like(y_te, np.nan)\n",
    "\n",
    "        for name, pred in {\"GLM\":yhat_glm, \"XGBoost\":yhat_xgb, \"CatBoost\":yhat_cb}.items():\n",
    "            m = metrics_log(y_te, pred)\n",
    "            rows.append(dict(task=\"A\", target=target, fold=fold, model=name, **m, n_test=int(len(y_te))))\n",
    "\n",
    "resA = pd.DataFrame(rows).sort_values([\"target\",\"model\",\"fold\"])\n",
    "resA_path = DATA_DIR / \"benchmark_results_growth.csv\"\n",
    "resA.to_csv(resA_path, index=False)\n",
    "resA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edb726",
   "metadata": {},
   "source": [
    "## 9) Reporting — aggregated tables & quick plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b486b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def summarize(res, keys=[\"scenario\",\"target\",\"model\"]):\n",
    "    grp = res.groupby(keys).agg(\n",
    "        rmse_log=(\"rmse_log\",\"mean\"),\n",
    "        mae_log=(\"mae_log\",\"mean\"),\n",
    "        r2=(\"r2\",\"mean\"),\n",
    "        mape=(\"mape\",\"mean\"),\n",
    "        n=(\"n_test\",\"sum\")\n",
    "    ).reset_index()\n",
    "    return grp\n",
    "\n",
    "# Summaries\n",
    "resB = pd.read_csv(DATA_DIR/\"benchmark_results_M.csv\")\n",
    "resA = pd.read_csv(DATA_DIR/\"benchmark_results_growth.csv\")\n",
    "\n",
    "sumB = summarize(resB, keys=[\"scenario\",\"model\"])\n",
    "sumA = summarize(resA, keys=[\"target\",\"model\"])\n",
    "\n",
    "display(sumB.head(10))\n",
    "display(sumA.head(10))\n",
    "\n",
    "# Example plot: RMSE by model (Task B, scenario B_full)\n",
    "sub = resB[resB[\"scenario\"]==\"B_full\"].groupby(\"model\")[\"rmse_log\"].mean().reset_index()\n",
    "plt.figure()\n",
    "plt.bar(sub[\"model\"], sub[\"rmse_log\"])\n",
    "plt.title(\"Task B (B_full): mean RMSE(log) by model\")\n",
    "plt.ylabel(\"RMSE(log)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cswae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
